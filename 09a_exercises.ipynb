{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1eb786-dd9a-4e20-bfa1-2c1154c3862d",
   "metadata": {},
   "source": [
    "# Exercises Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34bb92-f876-4dff-91d3-c6be14b9d6ee",
   "metadata": {},
   "source": [
    "1. How would you define clustering? Can you name a few clustering algorithms?\n",
    "\n",
    "A: Clustering means finding structures or groups of similar instances in the data (where the definition of \"similar\" varies from algorithm to algorithm). It's a type of unsupervised learning because it doesn't uses labels for training.\n",
    "\n",
    "Examples of these algorithms: K-means, Gaussian Mixtures, DBSCAN\n",
    "\n",
    "FROM THE APPENDIX: Examples of notions of similarity, sometimes similar means that instances are nearby (distance metric) while, for other algorithms, two instances are similar if they belong to the same densely packed group, even if they are far apart. Other algorithms: agglomerative clustering (hierarchical clustering), BIRCH, mean-shift, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469688e-a03b-431f-a22c-2ed1d4e74df3",
   "metadata": {},
   "source": [
    "2. What are some of the main applications of clustering algorithms?\n",
    "\n",
    "A: customer segmentation, identificating outliers or novel instances, propagating labels in semi-supervised training, orientate / prioritise data collection (we can use the most \"representative\" instances from each cluster), etc.\n",
    "\n",
    "FROM THE APPENDIX: image segmentation, dimensionality reduction (using the distances to the centroids as new dimensions), recommender systems, search engines, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a998a2b-3935-4502-bcf5-06ca2ee625f1",
   "metadata": {},
   "source": [
    "3. Describe two techniques to select the right number of clusters when using K-Means.\n",
    "\n",
    "A: the most common is the \"elbow method\" which consists in plotting the inertia of the model with different numbers of clusters (K) and then chosing the point in which the slope of the inertia changes (the inertia reduction slows down / the inertia stops dropping fast), a.k.a. \"the elbow\". Another (usually better) alternative is the sillhoute method, which consists in plotting the sillhouete against the number of clusters and picking K such that the sillhouete is maximised. The silhouette is defined as (b-a)/max(a,b), where b is the mean distance of the instance to those of the closest cluster and a is the mean distance to the instances in the same cluster. A good clustering should have all the clusters with average sillhouetes above zero and closer to 1.\n",
    "\n",
    "FROM THE APPENDIX: you can also plot a sillhouete diagram to perform a more thorough analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5022a-d01e-438e-abb6-3205839a1467",
   "metadata": {},
   "source": [
    "4. What is label propagation? Why would you implement it, and how?\n",
    "\n",
    "A: When we only have labels for a few instances, or it's very costly to get labels for all the instances, we can perform clustering in order to group the instances and then \"propagate\" labels based on the labelled instances in each cluster (or just invest resources on labelling those which are \"representative\" of each cluster). Using label propagation could lead to improvements in predictive performance vs. using just a few labeled instances.\n",
    "\n",
    "A consideration which we could have is to just propagate labels to instances that are closer to the centroid of each cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f64aa2-b9ad-46d5-8f96-f7ffa93dc632",
   "metadata": {},
   "source": [
    "5. Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?\n",
    "\n",
    "A: Accelerated K-means and Gaussian Mixtures (with appropriate constrains con var/covar) should scale well to large datasets. DBSCAN and mean-shift look for areas of high density, but they don't scale well to large datasets.\n",
    "\n",
    "FROM THE APPENDIX: BIRCH also scales well to large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70670f94-c29c-4e98-86c2-5e75d8477d74",
   "metadata": {},
   "source": [
    "6. Can you think of a use case where active learning would be useful? How would you implement it?\n",
    "\n",
    "A: It could be any case where labelling is expensive and/or requires human judgement. For example, identifying fruits that are not suitable for being sold at the store. We could start with just a handful of labelled instances, and then let the algorithm tell us which instances, if they had been labelled, would have lead to a greater reduction in metrics such as uncertainty or MSE, or have the potential to change the model the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9796094-888d-4ab3-b6fe-9308c7010b40",
   "metadata": {},
   "source": [
    "7. What is the difference between anomaly detection and novelty detection?\n",
    "\n",
    "A: I think the difference has to do with if the instances are in the training data (anomaly) or if they are new instances (novelty). In both cases they're \"unlikely\" instances given the training data structure.\n",
    "\n",
    "FROM THE APPENDIX: outliers/anomalies could be detected either in training data or new data, whereas novelties can only be on new data, and when we look for them we assume that the training data is clean (doesn't have outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f62cb-38a4-48fd-b661-22eb17ae1bc9",
   "metadata": {},
   "source": [
    "8. What is a Gaussian mixture? What tasks can you use it for?\n",
    "\n",
    "A: It's a combination of several gaussian distributions with different parameters (centroids/means and variances/covariances). A Gaussian mixture model assumes that the data comes from a combination of those distributions, and considers each one as a different cluster. Then, by an Expectation-Maximisation algorithm, it estimates the parameters of those distributions. \n",
    "\n",
    "FROM THE APPENDIX: the data is assumed to be grouped in a finite set of clusters, each one with ellipsoidal shape (they can have different orientation, size, and density), but we don't know which cluster each instance belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa7381-b008-4d55-adc6-ec522bedea1a",
   "metadata": {},
   "source": [
    "9. Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?\n",
    "\n",
    "A: Since the clusters in GMM can have different shapes and sizes, we can't use the inertia/elbow method or the sillhoute anymore. Two alternatives for picking the number of clusters are AIC and BIC (basically we're trying to maximise the likelihood of observing the actual instances given the estimated parameters).\n",
    "\n",
    "FROM THE APPENDIX: they also penalise the number of parameters (number of clusters). We should choose the model with the lower BIC or AIC. We could also use a Bayesian Gaussian mixture model, which \"automatically\" drops unnecessary clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonl-env:Python",
   "language": "python",
   "name": "conda-env-handsonl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
