{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd559a1c-3a50-44ad-9cb8-e1a1c3faa450",
   "metadata": {},
   "source": [
    "# Exercises 8 and 9 (Chapter 7)\n",
    "\n",
    "7. Load the MNIST data (introduced in Chapter 3), and:\n",
    "- split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing)\n",
    "- Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM classifier.\n",
    "- Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting.\n",
    "- Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e04252d-94f4-4695-95a7-36f3ae91d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113030e7-0004-475c-95d8-64e126f9ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea83842b-bf9b-46b3-b091-766ef4dfd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dadc4a-428e-476b-a95d-4456ef278a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_test, y_train_full, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full,\n",
    "                                                     test_size=10000,\n",
    "                                                     random_state=1989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fae65f-d30a-4db7-9601-736eea82fad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(n_estimators=500, n_jobs=-1),\n",
       "             param_grid={'max_leaf_nodes': [2, 22, 42, 62, 82, 102, 122, 142,\n",
       "                                            162, 182, 202, 222, 242, 262, 282,\n",
       "                                            302, 322, 342, 362, 382, 402, 422,\n",
       "                                            442, 462, 482, None]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train RandomForest, ExtraTrees and SVM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, # number of trees\n",
    "                                 n_jobs=-1) # use all the cores\n",
    "\n",
    "max_leaf_nodes_params = list(range(2, 500, 20))\n",
    "max_leaf_nodes_params.append(None)\n",
    "\n",
    "grid_rf = {\n",
    "    'max_leaf_nodes': max_leaf_nodes_params\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rnd_clf, grid_rf, cv = 3, scoring = 'accuracy')\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe7353-281c-4bdc-ad64-e4a69199b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51676883-4eb5-4d77-aaee-d2bc20aed41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the performance (score) on the validation set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_rf = grid_rf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3263d-e614-487b-b57a-038fa20bc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's the turn of an ExtraTrees classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "xt_clf = ExtraTreesClassifier(n_jobs=-1,\n",
    "                              random_state=1989)\n",
    "# max_leaf_nodes and n_estimators\n",
    "max_leaf_nodes_params = list(range(2, 500, 20))\n",
    "max_leaf_nodes_params.append(None)\n",
    "\n",
    "grid_xt = {\n",
    "    'max_leaf_nodes': max_leaf_nodes_params,\n",
    "    'n_estimators': list(range(100, 1000, 10))\n",
    "}\n",
    "\n",
    "gridsearch_xt = GridSearchCV(xt_clf,\n",
    "                            grid_xt,\n",
    "                            cv = 3,\n",
    "                            scoring = 'accuracy')\n",
    "\n",
    "gridsearch_xt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94b67a-a3f9-4258-88b6-f43982c7523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8c045-ffc1-48ff-8ce0-57fdaf7d888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xt = grid_search_xt.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398d43e-a572-4fa0-95d0-8d8a3bc3f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND FINALLY! THE SVM PREDICTOR\n",
    "# Look at the SVM chapter exercises\n",
    "# For this I have to scale and center the data\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", probability=True))\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0841c-abc0-45db-903d-81b9aa35f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336aa8e2-48ab-4ab9-afb5-2afcc0bf9549",
   "metadata": {},
   "source": [
    "Now I'm gonna combine them in a Voting Classifier.\n",
    "Note that if this is done with the default `VotingClassifier` class, it is going to retrain all the models. But here I want to compare the performance of them on their own with the one they get combined, so I need a voting classifier that preserves the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb02d3-ff5c-4cb9-b8c2-4ab0469fa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316feeaa-5b56-4aec-ad1b-85808aff9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution using mlxtend\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "import copy\n",
    "eclf_hard = EnsembleVoteClassifier(clfs=[grid_rf, grid_search_xt, svm_clf],\n",
    "                                   voting='hard',\n",
    "                                   fit_base_estimators=False)\n",
    "\n",
    "eclf_hard.fit(X_train, y_train)\n",
    "\n",
    "y_pred_vot_hard = eclf_hard.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_vot_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6239bb-c4d7-47ca-9c0a-3e650bfb7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf_soft = EnsembleVoteClassifier(clfs=[grid_rf, grid_search_xt, svm_clf],\n",
    "                                   voting='soft',\n",
    "                                   fit_base_estimators=False)\n",
    "\n",
    "ecl_soft.fit(X_train, y_train)\n",
    "\n",
    "y_pred_vot_soft = ecl_soft.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_vot_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08089d-7881-45c2-8479-27cb3ea84364",
   "metadata": {},
   "source": [
    "Now it's time to check the accuracy of the base models against the best voting classifier *on test data* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515793ce-46b1-4c81-af01-d89f9073d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Random Forest', 'ExtraTrees', 'SVM', 'Voting Classifier']\n",
    "\n",
    "for clf, label in zip([grid_rf, grid_search_xt, svm_clf, eclf_soft], labels):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %0.2f [%s]\" \n",
    "          % (score, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd7ee9-c539-4e81-9b1d-ca2f42f33c2c",
   "metadata": {},
   "source": [
    "9. \n",
    "\n",
    "- Run the individual classifiers from the previous exercise to make predictions on the validation set\n",
    "- create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class\n",
    "- Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble!\n",
    "- Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c43252-91bf-4258-8314-e28ae6572812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
