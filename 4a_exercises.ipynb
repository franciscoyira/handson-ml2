{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4632a83e-1ab4-4307-8636-7fb74782f9a9",
   "metadata": {},
   "source": [
    "# Exercises Chapter 4\n",
    "\n",
    "1. What Linear Regression algorithm should you use if you have million of features?\n",
    "\n",
    "A: I would use Gradient Descent, since it scales better when the number of features increase. I wouldn't mind using Batch GD, because they are not saying that the number of observations is large. If I wanted even better performance, I could use Mini Batch or stochastic gradient descent, but this would require to carefully set up a learning schedule in order to avoid the algorithm to wander around the global optima. \n",
    "\n",
    "\"Gradient Descent scales well with the number of features; training a Linear Regression model when there are hun‐ dreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8feb2-e971-4947-97db-c4c409171f0b",
   "metadata": {},
   "source": [
    "2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "A: In general, algorithms that use regularisation would have problems with variables in different scales. I could do normalisation/scale the variables (setting them up to have mean 0 and standard deviation equal to 1).\n",
    "\n",
    "ADDITIONALLY (from the book): gradient descent will take longer to converge if features are not scaled (cost function will have the shape of an elongated bowl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab100b-f05b-4dd9-854f-0f22f07be074",
   "metadata": {},
   "source": [
    "3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "\n",
    "A: No. Generally, GD could have this problem, but since the cost function of logistic regression is convex (bowl-shaped), there are no local minimums, only a global minimum that GD is guaranteed to reach (given enough time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63449d-33d8-4439-97da-eebcf01c01d5",
   "metadata": {},
   "source": [
    "4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "\n",
    "In Linear Regression, yes, but with other cost functions not necessarily. SGD and MBGD can escape local minimums in a way that BGD can't, so if the cost function has local minimums it's possible for these algorithms to reach different solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887545b-0702-45b1-8b42-7a7bb90e59e0",
   "metadata": {},
   "source": [
    "5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "A: It probably has started overfitting the training data (learning patterns that are not part of the data generating process but due to random chance / sample variance, so they don't generalise to the validation set). A possible solution here is using early stopping: keep the model trained in the epoch when the validation error reached a minimum.\n",
    "\n",
    "ADDITIONALLY: if the training error is also going up, it means that the learning rate is too high and the algorithm is diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fa717-675e-4934-95ac-a4145632df2c",
   "metadata": {},
   "source": [
    "6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "\n",
    "A: No, because due to its stochastic nature its to be expected that its validation error will sometimes go up just due to chance (sample variance of the mini-batches). A sensible approach would be to let the training continue until there is a clear upward trend in the validation error, and then roll-back the parameters/model to the point where it reached a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d1659-4dd8-47be-8e07-a2c2beb7a4a3",
   "metadata": {},
   "source": [
    "7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "A: The fastest should be SGD, since it uses just one observation in each iteration. However, once close to the optimum, it's likely to wander near it instead of settling down due to its stochastic nature. BGD will converge to the minimum without wandering (providing there are no local minimums along the way). In order to make SGD and MBGD converge too we should set a \"learning schedule\": gradually reducing the learning rate for each epoch/iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1f520-0abd-47dd-bb8f-076cc1de6e45",
   "metadata": {},
   "source": [
    "8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "A: Large gaps between train error and validation error hint overfitting problems. This could be addressed by collecting more data (to reduce sample variance) or reducing the complexity of the model by using a regularisation technique (lasso, ridge or elastic net)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c3901-90a5-4dae-a6d4-a789925b374d",
   "metadata": {},
   "source": [
    "9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularisation hyperparameter α or reduce it?\n",
    "\n",
    "A: That sounds like high bias / underfitting. The regularisation parameter should be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f683346-5140-4336-8386-023491591b01",
   "metadata": {},
   "source": [
    "10. Why would you want to use:\n",
    "\n",
    "- Ridge instead of regular LR: because it helps to avoid overfitting and to generalise better the model to new data.\n",
    "\n",
    "- Lasso instead of Ridge: because it performs feature selection by setting some weights/coefficients to zero\n",
    "\n",
    "- Elastic Net instead of Lasso: because Lasso may behave erratically when the number of features is higher than the number of training instances, or when there are strongly correlated features. ADDITIONALLY: if you want Lasso without the erratic behaviour, you can just use Elastic Net with l1_ratio close to 1 (e.g. 0.9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bb43f-5746-4dec-a953-db1c5ace4ce5",
   "metadata": {},
   "source": [
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "\n",
    "A: I would say two Logistic Regression classifiers. What we actually want is two outputs (and image can be outdoor/indoor AND also daytime/nighttime), and Softmax is multiclass but not multioutput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25120e5d-1a26-4149-9122-b2c3ec4aa100",
   "metadata": {},
   "source": [
    "12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn).\n",
    "\n",
    "(try to do this for 15 minutes, then look at the answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00154e2b-f144-4607-b24a-01b2f594c13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f451f34-60c6-4a95-8e84-87fdfde757cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
