{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises (Chapter 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?\n",
    "\n",
    "A: A reason why we would want to reduce dimensionality is to reduce the training and prediction time of prediction algorithms (in most cases computational complexity depends on N, the number of features, so by reducing it, the computational complexity is reduced too). Also, in some cases reducing the dimensionality improves the performance (accuracy) of the models too. However, this is not guaranteed to happen, and it's not the main reason why dimensionality reduction is usually performed.\n",
    "\n",
    "The main drawback of this technique is that it makes us lose some information (the dimensionality reduction algorithms try to minimise this information loss, but we'll inveitably lose some). Also, it adds complexity to the training and prediction pipeline.\n",
    "\n",
    "FROM THE APPENDIX:\n",
    "- Additional motivations: data visualisation (we can only visualise two or at most three features at a time, so it's cool to have 2 or 3 features that summarise most of the variation), and for data compression (e.g. with images or videos).\n",
    "\n",
    "- Additional drawbacks: can be computationally intensive, and trasformed features are sometimes hard to interpret (what does it mean an additional unit in the third component??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the curse of dimensionality\n",
    "\n",
    "A: In high dimensional spaces, observations tend to be very far away of each other (there are no \"close neighbours\") and also it becomes easier to find observations with extreme values in at least one dimension. This causes problems for several prediction algorithms, specially those that rely on similarity metrics.\n",
    "\n",
    "FROM THE APPENDIX: \"randomly-sampled high dimensional vectors tend to be very sparse, increasing the risk of overfitting\" and making the methods \"data greedy\" (requiring a lot of training data in order to archieve good results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
    "\n",
    "A: It depends on the method used for the dimensionality reduction. For many of them it is possible. For example, if we used PCA, restore the original dataset is straighforward: we just need to multiply the principal components and the rotation matrix. If we discarded some components, then what we get back is some sort of \"compressed\" version of the original data.\n",
    "\n",
    "However, with methods such as Kernal PCA with RBF kernel, it's not so easy. That's because the kernel transformation implies projecting the original data in an infinite-dimensional space at some point, thanks to the mathematical kernel trick. This infinite-dimensional space can't exist in memory, so \"going back\" to the original dataset/dimensionality is more complicated and requires some shenanigans.\n",
    "\n",
    "FROM THE APPENDIX: \"it's almost impossible\" (I think I was assuming we kept all the principal components from PCA). It also mentions that some methods, such as T-SNE don't have a straightforward reverse operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
    "\n",
    "A: Well, it CAN be used but probably it's probably going to have problems to capture the variance in the data. For these kinds of datasets it could be a better idea to use methods such as Kernel PCA.\n",
    "\n",
    "FROM THE APPENDIX: It can, and it would be useful to get rid off unnecessary dimensions while keeping most of the variance. The problem happens when there are no \"useless dimensions\" (e.g. Swiss roll dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
    "\n",
    "A: We don't know, it depends on the data structure.\n",
    "\n",
    "FROM THE APPENDIX: the range of neccessary dimensions (components) goes from 1 (perfectly aligned points) to 950 (randomly scattered points). Additionally, plotting the explained variance against the number of components is a useful way to get a rough idea of the intrinsic dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?\n",
    "\n",
    "A: I would use vanilla PCA if I had a relatively small dataset, Incremental PCA if I had a dataset so big that it doesn't fit in memory as a whole, and Kernel PCA if I had highly non-linear dataset.\n",
    "\n",
    "FROM THE APPENDIX: My answer was kind of good, actually. Incremental PCA is also used when we have to perform \"online\" PCA on the fly (updating the dimensionality reduction whenever a new instance arrives). Randomised PCA is used when the dataset fits in memory, but is big and we want to get a performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
    "\n",
    "A: If the dimensionality reduction is part of a prediction pipeline, then we can treat it as a hyperparameter and evaluate it through cross validation. If it's not, then we could measure some sort of \"reconstruction error\", try to recreate the original dataset with the original dimensionality from the reduced dataset, and measure the distance through metrics such as MSE.\n",
    "\n",
    "FROM THE APPENDIX: It's exactly what I wrote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Does it make any sense to chain two different dimensionality reduction algorithms?\n",
    "\n",
    "A: Honestly, I don't know.\n",
    "\n",
    "FROM THE APPENDIX: An example of when it makes sense to do this is using PCA to quickly get rid of a lot of useless/low-info dimensions and then applying a more complex/time-consuming algorithm on the result (e.g. LLE). This will produce similar results to just using the time-consumig algorithm, but in a fraction of the time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonl-env:Python",
   "language": "python",
   "name": "conda-env-handsonl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
