{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21caa9f9-8d32-4223-9e3d-7062042d9aaf",
   "metadata": {},
   "source": [
    "# Exercises Chapter 7\n",
    "\n",
    "1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "A: Yes, as long as they make *different* kind of errors, it's likely that they predictions combined are going to have higher accuracy on test data.\n",
    "\n",
    "The improvement in accuracy is going to be correlated with how uncorrelated are the errors in the different models (more uncorrelated means a higher increase in accuracy).\n",
    "\n",
    "NOT SURE ABOUT THIS BUT: I think that the point of training the models on different data is to make it more likely that the errors will be uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b08f0-4c4e-4709-9f0f-08477d4cd483",
   "metadata": {},
   "source": [
    "2. What is the difference between hard and soft voting classifiers?\n",
    "\n",
    "When doing hard voting, each predictor gives a full vote to one class (the one they \"think\" is more likely). But with soft voting we aggregate the predictors by averaging the classes probabilities. This is good because it takes into account how confident are the predictors about their \"votes\". However, for some algorithms (like SVM) it requires extra arguments and compute because they don't return class probabilities by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1249be-3c89-443c-99a2-aee323cf4f3f",
   "metadata": {},
   "source": [
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, Random Forests, or stacking ensembles?\n",
    "\n",
    "A: Bagging can be fully parallelised because the training of each model is independent of the other ones (bootstrapping to create subsamples). But boosting can't because each iteration/predictor needs an input from the previous models (it needs to know the errors of the previous model in order to give new weights to the training instances).\n",
    "\n",
    "Random Forests should be possible to parallelise too, because there isn't anything in the training of each tree that requires of the other trees as input. ERROR: Random Forest use bagging or pasting!\n",
    "\n",
    "With pasting we can parallelise the training but not the sampling, which doesn't matter that much because training is the most compute/time consuming part of the process.\n",
    "\n",
    "With Stacking we can parallelise some stages of the process, but other ones (such as the training of the aggregation layer) have dependencies, so the traning can't be completely parallelised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4421d21-5251-4dfa-a2dc-21d8278423d2",
   "metadata": {},
   "source": [
    "4. What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "A: It spares us from having a separate validation set or doing cross validation by leveraging the fact that, when doing bootstrapping, aproximately 33% of the instances won't be selected in a sample of size m (with replacement), i.e. they won't be selected in the bag. Then, we can ask each predictor for a prediction on the observations that weren't chosen for the respective training, thus obtaining a \"clean\" prediction on \"new data\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ad91e-20ee-4d38-ba5b-c74d7c3887b6",
   "metadata": {},
   "source": [
    "5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    "\n",
    "A: They're \"extra random\" because instead of optimising through k and t_k at each node (the feature to be used for the split and the cut-off) they only choose k and then pick t_k randomly. This trades more bias for less variance (model is less likely to overfit to our specific training sample). Also they're much faster to train than regular random forest because choosing t_k is one of the most time consuming steps of RF training.\n",
    "\n",
    "However, they don't always provide better results than regular Random Forests. It really depends on the dataset, so we can only be sure by testing them both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727f10e-8d1f-45a5-ae16-0b4838d1a517",
   "metadata": {},
   "source": [
    "6. If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?\n",
    "\n",
    "A: I could tweak the hyperparameters of the base learner (e.g. `max_depth` if its a Decision Tree) and also the `learning_rate` (increasing it to reduce regularisation and avoid underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a90402-8ac4-4e6f-a0d8-4cf637618d81",
   "metadata": {},
   "source": [
    "7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "\n",
    "A: decrease it, and maybe try some early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ca647-190d-4cb1-9d7d-2f39846147f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
