{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e1f484-58b0-414e-a895-94cbdc929810",
   "metadata": {},
   "source": [
    "# Exercises Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e2a62-ef9a-4d2e-9d2a-66892f3f21ca",
   "metadata": {},
   "source": [
    "1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?\n",
    "\n",
    "A: log_2(m) with m = 1000000 is between 19 and 20.\n",
    "\n",
    "IN ADDITION: this rule follows when the tree is balanced. But binary decision trees (those which have only two childs in each node) tend to be balanced. If we train this kind of tree without restrictions we'll end up with a leaf node per training instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c9aaf-09ad-4f6f-b51c-170d4d9eb718",
   "metadata": {},
   "source": [
    "2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it generally lower/greater, or always lower/greater?\n",
    "\n",
    "A: If I had to answer that, I would say that it's always lower, because the cost function is trying to minimise impurity, and at this moment I can't conceive an example of how it could increase.\n",
    "\n",
    "CORRECT ANSWER: It's generally lower, due to the cost function which tries to minimise the weighted average of child nodes impurity. This \"weighted average\" of impurity is always lower than the parent node impurity. HOWEVER, it could happen that a specific child node (one of the two, in the case of a binary tree) has higher impurity than its parent, as long as this is more than compensated with the impurity decrease in the other child node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfe81a-aab3-42d0-8052-6a1e27177748",
   "metadata": {},
   "source": [
    "3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?\n",
    "\n",
    "A: Yes. max_depth is a regularisation parameter that constrains how many node levels (depth) a tree can have. By reducing it we reduce the flexibility of the model and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ac2b2-7e59-47b5-9b82-cbec0919f5e8",
   "metadata": {},
   "source": [
    "4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "\n",
    "A: I don't think so. Scaling and centering is not relevant when training decision trees. Instead, we should try tuning the regularisation parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae18677-bf7e-4852-99ee-a8985a2e1f9a",
   "metadata": {},
   "source": [
    "5. If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?\n",
    "\n",
    "A: The computational complexity of training a decision tree is roughly O(n x log2(m)). So it should take roughly 16% more time, or 10 extra minutes.\n",
    "\n",
    "INCORRECT! The computational complexity of training a DT is O(n x m x log2(m)). Doing the math we could verify that the training on 10M would take 11.7 hours approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ff4c8-b69a-4f79-9637-fee1c0d1e4ca",
   "metadata": {},
   "source": [
    "6. If your training set contains 100,000 instances, will setting presort=True speed up training?\n",
    "\n",
    "A: Pre-sorting speeds up training in small datasets, but it's detrimental to performance in large datasets. I think 100.000 is more on the side of a large dataset, so I wouldn't use presorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8b86a-2ae9-487e-8aa4-a24fc790e2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
